# import re
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sentence_transformers import SentenceTransformer
#
# def normalize_text(text):
#     """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –¥–ª—è NLP."""
#     if not isinstance(text, str):
#         text = ""
#     text = text.lower()
#     text = re.sub(r'[\"‚Äú‚Äù]', '"', text)
#     text = re.sub(r'[^a-z–∞-—è—ë0-9\\s"\'.,()-]', ' ', text, flags=re.IGNORECASE)
#     text = re.sub(r'\\s+', ' ', text).strip()
#     return text
#
# def extract_keywords(text):
#     """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–µ."""
#     return ' '.join(re.findall(r'[–∞-—èa-z—ë]+', text.lower()))
#
# def compute_tfidf_similarity(masters, nomenclatures, ngram_range=(1,2), max_features=5000, min_df=2, max_df=0.8):
#     """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ø–∏—Å–∫–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF."""
#     to_compare = [extract_keywords(normalize_text(m)) + " " + extract_keywords(normalize_text(n)) for m, n in zip(masters, nomenclatures)]
#     vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features, min_df=min_df, max_df=max_df)
#     tfidf = vectorizer.fit_transform(to_compare)
#     similarities = (tfidf * tfidf.T).A
#     return [similarities[i, i] for i in range(len(masters))]
#
# def compute_sentence_similarity(masters, nomenclatures, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
#     """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é sentence-transformers."""
#     model = SentenceTransformer(model_name)
#     master_emb = model.encode([normalize_text(x) for x in masters])
#     nomenclature_emb = model.encode([normalize_text(x) for x in nomenclatures])
#     similarities = [float(model.similarity([m], [n])[0]) for m, n in zip(master_emb, nomenclature_emb)]
#     return similarities


# import re
import logging
# from typing import List
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
#
#
# class TextAnalyzer:
#     def __init__(self):
#         self.logger = logging.getLogger(__name__)
#
#     def normalize_text(self, text: str) -> str:
#         if not isinstance(text, str):
#             text = str(text)
#         text = text.lower()
#         text = re.sub(r'["""]', '"', text)
#         text = re.sub(r'[^a-z–∞-—è—ë0-9\s"\'.,()-]', ' ', text, flags=re.IGNORECASE)
#         text = re.sub(r'\s+', ' ', text).strip()
#         return text
#
#     def extract_keywords(self, text: str) -> str:
#         normalized = self.normalize_text(text)
#         keywords = re.findall(r'[–∞-—èa-z—ë]{3,}', normalized)
#         return ' '.join(keywords)
#
#     def analyze_similarity_batch(self, masters: List[str], nomenclatures: List[str]) -> List[float]:
#         try:
#             combined_texts = []
#             for master, nomenclature in zip(masters, nomenclatures):
#                 master_kw = self.extract_keywords(master)
#                 nomenclature_kw = self.extract_keywords(nomenclature)
#                 combined_texts.append(f"{master_kw} {nomenclature_kw}")
#
#             vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, min_df=1)
#             tfidf_matrix = vectorizer.fit_transform(combined_texts)
#
#             similarities = []
#             for i in range(len(masters)):
#                 master_vec = vectorizer.transform([self.extract_keywords(masters[i])])
#                 nomenclature_vec = vectorizer.transform([self.extract_keywords(nomenclatures[i])])
#                 similarity = cosine_similarity(master_vec, nomenclature_vec)[0][0]
#                 similarities.append(float(similarity))
#
#             return similarities
#         except Exception as e:
#             self.logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ: {e}")
#             return [0.0] * len(masters)


# import re
# import numpy as np
# from typing import List
#
#
# class TextAnalyzer:
#     def __init__(self):
#         self.logger = logging.getLogger(__name__)
#
#     def analyze_similarity_batch(self, masters: List[str], nomenclatures: List[str]) -> List[float]:
#         """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ä—É—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª"""
#         similarities = []
#
#         for master, nomenclature in zip(masters, nomenclatures):
#             # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
#             master_clean = self._semantic_normalize(master)
#             nomenclature_clean = self._semantic_normalize(nomenclature)
#
#             # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
#             master_words = self._get_semantic_words(master_clean)
#             nomenclature_words = self._get_semantic_words(nomenclature_clean)
#
#             # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
#             if master_words and nomenclature_words:
#                 intersection = len(master_words & nomenclature_words)
#                 union = len(master_words | nomenclature_words)
#                 similarity = intersection / union if union > 0 else 0.0
#             else:
#                 similarity = 0.0
#
#             similarities.append(similarity)
#
#         return similarities
#
#     def _semantic_normalize(self, text):
#         """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è"""
#         text = text.lower()
#         # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
#         text = re.sub(r'\d+[/-]\d+', '', text)
#         text = re.sub(r'\d+\s*(–Ω–º|–∫–≥|–º–º|–∫–≥–º)', '', text)
#         text = re.sub(r'[^a-z–∞-—è—ë\s]', ' ', text, flags=re.IGNORECASE)
#         return re.sub(r'\s+', ' ', text).strip()
#
#     def _get_semantic_words(self, text):
#         """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤"""
#         stop_words = {'–¥–ª—è', '–∏–ª–∏', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '—Ç–æ—Ç', '–µ–≥–æ', '–æ–Ω–∞'}
#         words = set(re.findall(r'[–∞-—èa-z—ë]{3,}', text))
#         return words - stop_words

# import re
# import logging
# from typing import List
#
#
# class TextAnalyzer:
#     """–†–ê–ë–û–¢–ê–Æ–©–ò–ô –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π"""
#
#     def __init__(self):
#         self.logger = logging.getLogger(__name__)
#
#     def analyze_similarity_batch(self, masters: List[str], nomenclatures: List[str]) -> List[float]:
#         """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
#         self.logger.info(f"–ê–Ω–∞–ª–∏–∑ {len(masters)} –ø–∞—Ä...")
#
#         similarities = []
#         for i, (master, nomenclature) in enumerate(zip(masters, nomenclatures)):
#             similarity = self._compute_real_similarity(master, nomenclature)
#             similarities.append(similarity)
#
#             # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
#             if i < 5:
#                 self.logger.info(f"–ü–∞—Ä–∞ {i + 1}: —Å—Ö–æ–∂–µ—Å—Ç—å = {similarity:.3f}")
#
#         return similarities
#
#     def _compute_real_similarity(self, master: str, nomenclature: str) -> float:
#         """–†–µ–∞–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
#
#         # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
#         master_terms = self._extract_key_terms(master)
#         nomenclature_terms = self._extract_key_terms(nomenclature)
#
#         # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ - –Ω–∏–∑–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
#         if not master_terms or not nomenclature_terms:
#             return 0.1
#
#         # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
#         intersection = len(master_terms & nomenclature_terms)
#         union = len(master_terms | nomenclature_terms)
#
#         return intersection / union if union > 0 else 0.1
#
#     def _extract_key_terms(self, text: str) -> set:
#         """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
#         text = text.lower()
#
#         # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
#         replacements = {
#             '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–æ—Ç–∫–∞': '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–µ—Ç–∫–∞',  # –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏
#             '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π': '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
#             '–≥–∞–µ—á–Ω—ã–π': '–∫–ª—é—á–µ–≤–æ–π',
#             '–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π': '',
#         }
#
#         # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
#         for old, new in replacements.items():
#             text = text.replace(old, new)
#
#         # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏ —á–∏—Å–ª–∞
#         text = re.sub(r'\d+[/-]\d+', '', text)  # —Ä–∞–∑–º–µ—Ä—ã —Ç–∏–ø–∞ 3/8
#         text = re.sub(r'\d+\s*(–Ω–º|–∫–≥|–º–º|–∫–≥–º)', '', text)  # –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
#         text = re.sub(r'[^a-z–∞-—è—ë\s]', ' ', text)  # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
#         text = re.sub(r'\s+', ' ', text).strip()
#
#         # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
#         key_patterns = [
#             r'–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–µ—Ç–∫–∞', r'–∫–ª—é—á', r'–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', r'–∫–ª—é—á–µ–≤–æ–π',
#             r'—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å', r'—à–ª–∞–Ω–≥', r'–ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–∏–µ', r'—É–¥–∞—Ä–Ω—ã–π',
#             r'—É–∫–æ—Ä–æ—á–µ–Ω–Ω–∞—è', r'–∫–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è', r'–∫–≤–∞–¥—Ä–∞—Ç'
#         ]
#
#         terms = set()
#         for pattern in key_patterns:
#             if re.search(pattern, text):
#                 terms.add(pattern.replace('r\\', ''))
#
#         # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
#         if not terms:
#             words = re.findall(r'[–∞-—èa-z—ë]{4,}', text)
#             terms = set(words[:3])  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞
#
#         return terms


# from enhanced_text_analyzer import EnhancedTextAnalyzer
#
#
# def main():
#     analyzer = EnhancedTextAnalyzer()
#     similarities = analyzer.analyze_similarity_batch(masters, nomenclatures)
#
#     # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
#     threshold = analyzer.get_anomaly_threshold(similarities, method='iqr')
#
#     # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
#     anomalies = post_process_anomalies(anomalies, analyzer)
#
#     # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
#     anomalies['anomaly_type'] = classify_anomalies(anomalies)
#     anomalies['confidence'] = 1 - anomalies['similarity_score']

# import re
# import numpy as np
# from typing import List, Set
#
#
# class SuperEnhancedTextAnalyzer:
#     def __init__(self):
#         self.logger = logging.getLogger(__name__)
#
#         # –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–õ–û–í–ê–†–¨ –°–ò–ù–û–ù–ò–ú–û–í (200+ –ø–∞—Ä)
#         self.technical_synonyms = {
#             # –û–°–ù–û–í–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´
#             '–æ—Ç–≤–µ—Ä—Ç–∫–∞': '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è',
#             '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è': '–æ—Ç–≤–µ—Ä—Ç–∫–∞',
#             '–±–∏—Ç–∞': '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è',
#             '–±–∏—Ç—ã': '–æ—Ç–≤–µ—Ä—Ç–∫–∏',
#
#             # –¢–ò–ü–´ –û–¢–í–ï–†–¢–û–ö
#             '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è': 'phillips',
#             'phillips': '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è',
#             'ph': 'phillips',
#             'pz': 'pozidriv',
#             'pozidriv': 'pz',
#             '—à–ª–∏—Ü–µ–≤–∞—è': 'slotted',
#             'slotted': '—à–ª–∏—Ü–µ–≤–∞—è',
#             'sl': 'slotted',
#             '–ø—Ä—è–º–∞—è': 'slotted',
#
#             # HEX –ò TORX
#             '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫': 'hex',
#             '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è': 'hex',
#             '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–π': 'hex',
#             '–∏–º–±—É—Å': 'hex',
#             'hex': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
#             '—Ç–æ—Ä–∫—Å': 'torx',
#             'torx': '—Ç–æ—Ä–∫—Å',
#             '–∑–≤–µ–∑–¥–æ—á–∫–∞': 'torx',
#             '–∑–≤–µ–∑–¥–æ–æ–±—Ä–∞–∑–Ω–∞—è': 'torx',
#
#             # –ö–õ–Æ–ß–ò
#             '–∫–ª—é—á': '–≥–∞–µ—á–Ω—ã–π',
#             '–≥–∞–µ—á–Ω—ã–π': '–∫–ª—é—á',
#             '—Ä–æ–∂–∫–æ–≤—ã–π': '–æ—Ç–∫—Ä—ã—Ç—ã–π',
#             '–æ—Ç–∫—Ä—ã—Ç—ã–π': '—Ä–æ–∂–∫–æ–≤—ã–π',
#             '–Ω–∞–∫–∏–¥–Ω–æ–π': '–∫–æ–ª—å—Ü–µ–≤–æ–π',
#             '–∫–æ–ª—å—Ü–µ–≤–æ–π': '–Ω–∞–∫–∏–¥–Ω–æ–π',
#             '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π': '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
#             '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π': '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π',
#             '—Ä–∞–∑–≤–æ–¥–Ω–æ–π': 'adjustable',
#             'adjustable': '—Ä–∞–∑–≤–æ–¥–Ω–æ–π',
#
#             # –ì–û–õ–û–í–ö–ò –ò –ù–ê–°–ê–î–ö–ò
#             '–≥–æ–ª–æ–≤–∫–∞': '–Ω–∞—Å–∞–¥–∫–∞',
#             '–Ω–∞—Å–∞–¥–∫–∞': '–≥–æ–ª–æ–≤–∫–∞',
#             '—Ç–æ—Ä—Ü–µ–≤–∞—è': '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è',
#             '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è': '—Ç–æ—Ä—Ü–µ–≤–∞—è',
#             '—Ç–æ—Ä—Ü–æ–≤–∞—è': '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è',
#             'socket': '–≥–æ–ª–æ–≤–∫–∞',
#
#             # –£–î–õ–ò–ù–ò–¢–ï–õ–ò
#             '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å': '–∫–∞—Ä–¥–∞–Ω–Ω—ã–π',
#             '–∫–∞—Ä–¥–∞–Ω–Ω—ã–π': '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å',
#             '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫': '–∞–¥–∞–ø—Ç–µ—Ä',
#             '–∞–¥–∞–ø—Ç–µ—Ä': '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫',
#             'extension': '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å',
#
#             # –°–¢–†–£–ë–¶–ò–ù–´
#             '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞': '–∑–∞–∂–∏–º',
#             '–∑–∞–∂–∏–º': '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
#             '—Ç–∏—Å–∫–∏': '–∑–∞–∂–∏–º',
#             'clamp': '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
#             'g-–æ–±—Ä–∞–∑–Ω–∞—è': 'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
#             'f-–æ–±—Ä–∞–∑–Ω–∞—è': 'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
#
#             # –ü–ê–°–°–ê–¢–ò–ñ–ò
#             '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã': '–ø–∞—Å—Å–∞—Ç–∏–∂–∏',
#             '–ø–∞—Å—Å–∞—Ç–∏–∂–∏': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
#             '–∫—É—Å–∞—á–∫–∏': '–±–æ–∫–æ—Ä–µ–∑—ã',
#             '–±–æ–∫–æ—Ä–µ–∑—ã': '–∫—É—Å–∞—á–∫–∏',
#             '–∫—Ä—É–≥–ª–æ–≥—É–±—Ü—ã': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
#             '–¥–ª–∏–Ω–Ω–æ–≥—É–±—Ü—ã': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
#
#             # –ú–û–õ–û–¢–ö–ò
#             '–º–æ–ª–æ—Ç–æ–∫': '–∫—É–≤–∞–ª–¥–∞',
#             '–∫—É–≤–∞–ª–¥–∞': '–º–æ–ª–æ—Ç–æ–∫',
#             '–∫–∏—è–Ω–∫–∞': '–º–æ–ª–æ—Ç–æ–∫',
#             'hammer': '–º–æ–ª–æ—Ç–æ–∫',
#
#             # –ü–ù–ï–í–ú–ê–¢–ò–ß–ï–°–ö–ò–ï
#             '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–æ—Ç–∫–∞': '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–µ—Ç–∫–∞',
#             '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π': '–ø–Ω–µ–≤–º–æ',
#             '–ø–Ω–µ–≤–º–æ': '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π',
#             '—Ç—Ä–µ—â–æ—Ç–∫–∞': '—Ç—Ä–µ—â–µ—Ç–∫–∞',
#             '—Ç—Ä–µ—â–µ—Ç–∫–∞': '—Ç—Ä–µ—â–æ—Ç–∫–∞',
#             'ratchet': '—Ç—Ä–µ—â–æ—Ç–∫–∞',
#
#             # –ú–ê–¢–ï–†–ò–ê–õ–´
#             '—Ö—Ä–æ–º–≤–∞–Ω–∞–¥–∏–µ–≤–∞—è': 'crv',
#             '—Ö—Ä–æ–º-–≤–∞–Ω–∞–¥–∏–π': 'crv',
#             'chrome-vanadium': 'crv',
#             'cr-v': 'crv',
#             'crv': '—Ö—Ä–æ–º-–≤–∞–Ω–∞–¥–∏–π',
#             'crmo': '—Ö—Ä–æ–º-–º–æ–ª–∏–±–¥–µ–Ω',
#             'cr-mo': '—Ö—Ä–æ–º-–º–æ–ª–∏–±–¥–µ–Ω',
#             'chrome-molybdenum': 'crmo',
#
#             # –ü–û–ö–†–´–¢–ò–Ø
#             '—Ö—Ä–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è': 'chrome',
#             'chrome': '—Ö—Ä–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è',
#             '–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è': 'polished',
#             'polished': '–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è',
#             '–º–∞—Ç–æ–≤–∞—è': 'matt',
#             'matt': '–º–∞—Ç–æ–≤–∞—è',
#
#             # –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø
#             'vde': '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
#             '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π': 'vde',
#             'anti-slip': '–ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∑—è—â–∏–π',
#             '–ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∑—è—â–∏–π': 'anti-slip',
#             'grip': '—Ä—É–∫–æ—è—Ç–∫–∞',
#             '—Ä—É–∫–æ—è—Ç–∫–∞': 'grip',
#             'ergonic': '—ç—Ä–≥–æ–Ω–æ–º–∏—á–Ω—ã–π',
#             '—ç—Ä–≥–æ–Ω–æ–º–∏—á–Ω—ã–π': 'ergonic',
#
#             # –†–ê–ó–ú–ï–†–´ (–¥—é–π–º—ã –≤ –º–º)
#             '1/4': '6.35',
#             '5/16': '7.94',
#             '3/8': '9.53',
#             '7/16': '11.11',
#             '1/2': '12.70',
#             '9/16': '14.29',
#             '5/8': '15.88',
#             '11/16': '17.46',
#             '3/4': '19.05',
#             '7/8': '22.23',
#
#             # –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ü–†–û–§–ò–õ–ò
#             'spline': '—à–ª–∏—Ü–µ–≤–æ–π',
#             '—à–ª–∏—Ü–µ–≤–æ–π': 'spline',
#             'square': '–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π',
#             '–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π': 'square',
#             '–∫–≤–∞–¥—Ä–∞—Ç': 'square',
#
#             # –≠–õ–ï–ö–¢–†–ò–ß–ï–°–ö–ò–ï
#             'awg': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π-–∫–∞–ª–∏–±—Ä',
#             '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π-–∫–∞–ª–∏–±—Ä': 'awg',
#             'wire': '–ø—Ä–æ–≤–æ–¥',
#             '–ø—Ä–æ–≤–æ–¥': 'wire',
#             'cable': '–∫–∞–±–µ–ª—å',
#             '–∫–∞–±–µ–ª—å': 'cable',
#
#             # –ë–£–†–´ –ò –°–í–ï–†–õ–ê
#             'sds-max': 'sds-–º–∞–∫—Å',
#             'sds-–º–∞–∫—Å': 'sds-max',
#             'sds-plus': 'sds-–ø–ª—é—Å',
#             'sds-–ø–ª—é—Å': 'sds-plus',
#             'drill': '—Å–≤–µ—Ä–ª–æ',
#             '—Å–≤–µ—Ä–ª–æ': 'drill',
#             '–±—É—Ä': 'drill',
#         }
#
#         # –ë—Ä–µ–Ω–¥—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ
#         self.brands = {
#             'kingtony', 'king-tony', 'jtc', 'matrix', 'gross', 'stayer',
#             'fit', 'kraftool', '–∑—É–±—Ä', '–≤–∏—Ö—Ä—å', '–ø–∞—Ç—Ä–∏–æ—Ç', '–∫–∞–ª–∏–±—Ä',
#             '–∫—Ä–∞—Ç–æ–Ω', '–∏–Ω—Ç–µ—Ä—Å–∫–æ–ª', 'skrab', 'rockforce', 'forsage',
#             'berger', 'hans', 'stanley', 'bosch', 'makita', 'sparta',
#             'garwin', 'stalex', 'ferroplex', 'simplex', 'hercules'
#         }
#
#     def normalize_technical_designations(self, text):
#         """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π"""
#         text = text.lower()
#
#         # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤
#         # PH1, PH2, PH3 ‚Üí phillips + —Ä–∞–∑–º–µ—Ä
#         text = re.sub(r'ph\s*(\d+)', r'phillips \1', text)
#         # SL3, SL4, SL5 ‚Üí slotted + —Ä–∞–∑–º–µ—Ä
#         text = re.sub(r'sl\s*(\d+(?:[.,]\d+)?)', r'slotted \1', text)
#         # T10, T20, T40 ‚Üí torx + —Ä–∞–∑–º–µ—Ä
#         text = re.sub(r't(\d+)', r'torx \1', text)
#         # H3, H4, H8 ‚Üí hex + —Ä–∞–∑–º–µ—Ä
#         text = re.sub(r'h(\d+)', r'hex \1', text)
#         # PZ1, PZ2 ‚Üí pozidriv + —Ä–∞–∑–º–µ—Ä
#         text = re.sub(r'pz\s*(\d+)', r'pozidriv \1', text)
#
#         # G-–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–±—Ü–∏–Ω—ã: G-75 ‚Üí g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ 75
#         text = re.sub(r'g-?\s*(\d+)', r'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ \1', text)
#         # F-–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–±—Ü–∏–Ω—ã: F-300 ‚Üí f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ 300
#         text = re.sub(r'f-?\s*(\d+)', r'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ \1', text)
#
#         return text
#
#     def analyze_similarity_batch(self, masters, nomenclatures):
#         """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
#         similarities = []
#
#         for i, (master, nomenclature) in enumerate(zip(masters, nomenclatures)):
#             # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è
#             master_norm = self.normalize_technical_designations(master)
#             nomenclature_norm = self.normalize_technical_designations(nomenclature)
#
#             # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑
#             semantic_score = self._enhanced_semantic_similarity(master_norm, nomenclature_norm)
#             size_score = self._enhanced_size_similarity(master, nomenclature)
#             category_score = self._category_similarity(master, nomenclature)
#
#             # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (70% —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + 15% —Ä–∞–∑–º–µ—Ä + 15% –∫–∞—Ç–µ–≥–æ—Ä–∏—è)
#             total_score = semantic_score * 0.7 + size_score * 0.15 + category_score * 0.15
#             similarities.append(min(1.0, max(0.0, total_score)))
#
#             if i % 1000 == 0 and i > 0:
#                 print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(masters)} –ø–∞—Ä")
#
#         return similarities
#
#     def _enhanced_semantic_similarity(self, master, nomenclature):
#         """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
#         master_terms = self._extract_enhanced_terms(master)
#         nomenclature_terms = self._extract_enhanced_terms(nomenclature)
#
#         if not master_terms or not nomenclature_terms:
#             return 0.1
#
#         # –ë–∞–∑–æ–≤–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
#         intersection = len(master_terms & nomenclature_terms)
#         union = len(master_terms | nomenclature_terms)
#         base_similarity = intersection / union if union > 0 else 0.0
#
#         # –ë–æ–Ω—É—Å –∑–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
#         functional_bonus = self._calculate_functional_bonus(master_terms, nomenclature_terms)
#
#         return min(1.0, base_similarity + functional_bonus)
#
#     def _calculate_functional_bonus(self, master_terms, nomenclature_terms):
#         """–ë–æ–Ω—É—Å –∑–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ"""
#         bonus = 0.0
#
#         # –ì—Ä—É–ø–ø—ã —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
#         functional_groups = [
#             {'phillips', '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è', 'ph'},
#             {'slotted', '—à–ª–∏—Ü–µ–≤–∞—è', 'sl', '–ø—Ä—è–º–∞—è'},
#             {'torx', '—Ç–æ—Ä–∫—Å', '–∑–≤–µ–∑–¥–æ—á–∫–∞'},
#             {'hex', '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫', '–∏–º–±—É—Å'},
#             {'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞', 'g-–æ–±—Ä–∞–∑–Ω–∞—è', '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞'},
#             {'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞', 'f-–æ–±—Ä–∞–∑–Ω–∞—è', '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞'},
#             {'–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π'},
#             {'–æ—Ç–≤–µ—Ä—Ç–∫–∞', '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è', '–±–∏—Ç–∞'},
#             {'–º–æ–ª–æ—Ç–æ–∫', '–∫—É–≤–∞–ª–¥–∞', '–∫–∏—è–Ω–∫–∞'},
#             {'–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã', '–ø–∞—Å—Å–∞—Ç–∏–∂–∏', '–∫—É—Å–∞—á–∫–∏'}
#         ]
#
#         for group in functional_groups:
#             master_has = bool(master_terms & group)
#             nomenclature_has = bool(nomenclature_terms & group)
#
#             if master_has and nomenclature_has:
#                 bonus += 0.3  # –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
#
#         return bonus
#
#     def _extract_enhanced_terms(self, text):
#         """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
#         # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
#         for old, new in self.technical_synonyms.items():
#             text = text.replace(old, new)
#
#         # –£–±–∏—Ä–∞–µ–º –±—Ä–µ–Ω–¥—ã
#         for brand in self.brands:
#             text = re.sub(r'\b' + re.escape(brand) + r'\b', '', text)
#
#         # –£–±–∏—Ä–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª—ã –∏ –∫–æ–¥—ã
#         text = re.sub(r'\b\d+[a-z]+-?\d+[a-z]*\b', '', text)  # 8PK-301F
#         text = re.sub(r'\b[a-z]+-?\d+[a-z]*\b', '', text)  # CP-371V
#
#         # –û—á–∏—Å—Ç–∫–∞
#         text = re.sub(r'[^a-z–∞-—è—ë0-9\s-]', ' ', text)
#         text = re.sub(r'\s+', ' ', text).strip()
#
#         # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
#         terms = set()
#         words = text.split()
#
#         for word in words:
#             if len(word) > 2 and word not in {'–¥–ª—è', '–∏–ª–∏', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '–º–º', '—Å–º'}:
#                 terms.add(word)
#
#         return terms


import sys
import os
import numpy as np
import pandas as pd
import re
import logging


class UltraEnhancedTextAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)


        # –û–ë–™–ï–î–ò–ù–ï–ù–ù–´–ô –°–õ–û–í–ê–†–¨ –°–ò–ù–û–ù–ò–ú–û–í (–í–ê–® + –ù–û–í–´–ï –ò–ó –§–ê–ô–õ–ê)
        self.technical_synonyms = {
            # –í–°–ï –í–ê–®–ò –°–ò–ù–û–ù–ò–ú–´ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é!)
            '–æ—Ç–≤–µ—Ä—Ç–∫–∞': '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è',
            '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è': '–æ—Ç–≤–µ—Ä—Ç–∫–∞',
            '–±–∏—Ç–∞': '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è',
            '–±–∏—Ç—ã': '–æ—Ç–≤–µ—Ä—Ç–∫–∏',
            '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è': 'phillips',
            'phillips': '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è',
            'ph': 'phillips',
            'pz': 'pozidriv',
            'pozidriv': 'pz',
            '—à–ª–∏—Ü–µ–≤–∞—è': 'slotted',
            'slotted': '—à–ª–∏—Ü–µ–≤–∞—è',
            'sl': 'slotted',
            '–ø—Ä—è–º–∞—è': 'slotted',
            '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫': 'hex',
            '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∞—è': 'hex',
            '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω—ã–π': 'hex',
            '–∏–º–±—É—Å': 'hex',
            'hex': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
            '—Ç–æ—Ä–∫—Å': 'torx',
            'torx': '—Ç–æ—Ä–∫—Å',
            '–∑–≤–µ–∑–¥–æ—á–∫–∞': 'torx',
            '–∑–≤–µ–∑–¥–æ–æ–±—Ä–∞–∑–Ω–∞—è': 'torx',
            '–∫–ª—é—á': '–≥–∞–µ—á–Ω—ã–π',
            '–≥–∞–µ—á–Ω—ã–π': '–∫–ª—é—á',
            '—Ä–æ–∂–∫–æ–≤—ã–π': '–æ—Ç–∫—Ä—ã—Ç—ã–π',
            '–æ—Ç–∫—Ä—ã—Ç—ã–π': '—Ä–æ–∂–∫–æ–≤—ã–π',
            '–Ω–∞–∫–∏–¥–Ω–æ–π': '–∫–æ–ª—å—Ü–µ–≤–æ–π',
            '–∫–æ–ª—å—Ü–µ–≤–æ–π': '–Ω–∞–∫–∏–¥–Ω–æ–π',
            '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π': '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π': '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π',
            '—Ä–∞–∑–≤–æ–¥–Ω–æ–π': 'adjustable',
            'adjustable': '—Ä–∞–∑–≤–æ–¥–Ω–æ–π',
            '–≥–æ–ª–æ–≤–∫–∞': '–Ω–∞—Å–∞–¥–∫–∞',
            '–Ω–∞—Å–∞–¥–∫–∞': '–≥–æ–ª–æ–≤–∫–∞',
            '–≥–æ–ª–æ–≤–∫–∏': '–Ω–∞–±–æ—Ä –≥–æ–ª–æ–≤–æ–∫',
            '—Ç–æ—Ä—Ü–µ–≤–∞—è': '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è',
            '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è': '—Ç–æ—Ä—Ü–µ–≤–∞—è',
            '—Ç–æ—Ä—Ü–æ–≤–∞—è': '–≥–æ–ª–æ–≤–æ—á–Ω–∞—è',
            'socket': '–≥–æ–ª–æ–≤–∫–∞',
            '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å': '–∫–∞—Ä–¥–∞–Ω–Ω—ã–π',
            '–∫–∞—Ä–¥–∞–Ω–Ω—ã–π': '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å',
            '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫': '–∞–¥–∞–ø—Ç–µ—Ä',
            '–∞–¥–∞–ø—Ç–µ—Ä': '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫',
            'extension': '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å',
            '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞': '–∑–∞–∂–∏–º',
            '–∑–∞–∂–∏–º': '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
            '—Ç–∏—Å–∫–∏': '–∑–∞–∂–∏–º',
            'clamp': '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
            'g-–æ–±—Ä–∞–∑–Ω–∞—è': 'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
            'f-–æ–±—Ä–∞–∑–Ω–∞—è': 'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞',
            '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã': '–ø–∞—Å—Å–∞—Ç–∏–∂–∏',
            '–ø–∞—Å—Å–∞—Ç–∏–∂–∏': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
            '–∫—É—Å–∞—á–∫–∏': '–±–æ–∫–æ—Ä–µ–∑—ã',
            '–±–æ–∫–æ—Ä–µ–∑—ã': '–∫—É—Å–∞—á–∫–∏',
            '–∫—Ä—É–≥–ª–æ–≥—É–±—Ü—ã': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
            '–¥–ª–∏–Ω–Ω–æ–≥—É–±—Ü—ã': '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã',
            '–º–æ–ª–æ—Ç–æ–∫': '–∫—É–≤–∞–ª–¥–∞',
            '–∫—É–≤–∞–ª–¥–∞': '–º–æ–ª–æ—Ç–æ–∫',
            '–∫–∏—è–Ω–∫–∞': '–º–æ–ª–æ—Ç–æ–∫',
            'hammer': '–º–æ–ª–æ—Ç–æ–∫',
            '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–æ—Ç–∫–∞': '–ø–Ω–µ–≤–º–æ—Ç—Ä–µ—â–µ—Ç–∫–∞',
            '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π': '–ø–Ω–µ–≤–º–æ',
            '–ø–Ω–µ–≤–º–æ': '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π',
            '—Ç—Ä–µ—â–æ—Ç–∫–∞': '—Ç—Ä–µ—â–µ—Ç–∫–∞',
            '—Ç—Ä–µ—â–µ—Ç–∫–∞': '—Ç—Ä–µ—â–æ—Ç–∫–∞',
            'ratchet': '—Ç—Ä–µ—â–æ—Ç–∫–∞',

            # –í–°–ï –í–ê–®–ò –ú–ê–¢–ï–†–ò–ê–õ–´ –ò –ü–û–ö–†–´–¢–ò–Ø
            '—Ö—Ä–æ–º–≤–∞–Ω–∞–¥–∏–µ–≤–∞—è': 'crv',
            '—Ö—Ä–æ–º-–≤–∞–Ω–∞–¥–∏–π': 'crv',
            'chrome-vanadium': 'crv',
            'cr-v': 'crv',
            'crv': '—Ö—Ä–æ–º-–≤–∞–Ω–∞–¥–∏–π',
            'crmo': '—Ö—Ä–æ–º-–º–æ–ª–∏–±–¥–µ–Ω',
            'cr-mo': '—Ö—Ä–æ–º-–º–æ–ª–∏–±–¥–µ–Ω',
            'chrome-molybdenum': 'crmo',
            '—Ö—Ä–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è': 'chrome',
            'chrome': '—Ö—Ä–æ–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è',
            '–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è': 'polished',
            'polished': '–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è',
            '–º–∞—Ç–æ–≤–∞—è': 'matt',
            'matt': '–º–∞—Ç–æ–≤–∞—è',
            'vde': '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π': 'vde',
            'anti-slip': '–ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∑—è—â–∏–π',
            '–ø—Ä–æ—Ç–∏–≤–æ—Å–∫–æ–ª—å–∑—è—â–∏–π': 'anti-slip',
            'grip': '—Ä—É–∫–æ—è—Ç–∫–∞',
            '—Ä—É–∫–æ—è—Ç–∫–∞': 'grip',
            'ergonic': '—ç—Ä–≥–æ–Ω–æ–º–∏—á–Ω—ã–π',
            '—ç—Ä–≥–æ–Ω–æ–º–∏—á–Ω—ã–π': 'ergonic',
            '–∫—Ä–∏–º–ø–µ—Ä': '–∫–ª–µ—â–∏',
            '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ': '–∏–∑–æ–ª—è—Ü–∏–∏',

            # –£–õ–£–ß–®–ï–ù–ù–´–ï –°–ò–ù–û–ù–ò–ú–´ –î–õ–Ø –ö–†–ò–ú–ü–ï–†–û–í/–≠–õ–ï–ö–¢–†–û–ú–û–ù–¢–ê–ñ–ù–´–•
            '–∫—Ä–∏–º–ø–µ—Ä': '–æ–±–∂–∏–º–Ω—ã–µ',
            '–æ–±–∂–∏–º–Ω—ã–µ': '–∫—Ä–∏–º–ø–µ—Ä',
            '–æ–±–∂–∏–º–Ω–∏–∫': '–∫—Ä–∏–º–ø–µ—Ä',
            '–æ–±–∂–∏–º–∞—Ç–µ–ª—å': '–∫—Ä–∏–º–ø–µ—Ä',
            '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ': '–∫—Ä–∏–º–ø–µ—Ä',
            '–∫–ª–µ—â–∏ —ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ': '–∫—Ä–∏–º–ø–µ—Ä',
            '–ø—Ä–µ—Å—Å-–∫–ª–µ—â–∏': '–∫—Ä–∏–º–ø–µ—Ä',
            '—Å—Ç—Ä–∏–ø–ø–µ—Ä': '–∫—Ä–∏–º–ø–µ—Ä',  # –¢–∞–∫–∂–µ –∫—Ä–∏–º–ø–µ—Ä
            '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±–∂–∏–º–∞': '–∫—Ä–∏–º–ø–µ—Ä',
            '–∫—Ä–∏–º–ø–µ—Ä —Ö—Ä–∞–ø–æ–≤–∏—á–Ω—ã–π': '–∫—Ä–∏–º–ø–µ—Ä',
            '–æ–±–∂–∏–º–Ω—ã–µ –∫–ª–µ—â–∏': '–∫—Ä–∏–º–ø–µ—Ä',
            '—Å–µ–∫—Ç–æ—Ä–Ω—ã–π': '—Å–µ–∫—Ç–æ—Ä–Ω—ã–µ',
            '–º—É–ª—å—Ç–∏—Ç—É–ª': '–º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π',
            '–º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π': '–º—É–ª—å—Ç–∏—Ç—É–ª',
            '–∑–∞–∫–ª–µ–ø–æ—á–Ω–∏–∫': '–∑–∞–∫–ª—ë–ø–æ—á–Ω–∏–∫',
            '–∑–∞–∫–ª—ë–ø–æ—á–Ω–∏–∫': '–∑–∞–∫–ª–µ–ø–æ—á–Ω–∏–∫',
            '—Å–µ–∫—Ç–æ—Ä–Ω—ã–µ': '—Å–µ–∫—Ç–æ—Ä–Ω—ã–π',


            # –í–°–ï –í–ê–®–ò –†–ê–ó–ú–ï–†–´
            '1/4': '6.35',
            '5/16': '7.94',
            '3/8': '9.53',
            '7/16': '11.11',
            '1/2': '12.70',
            '9/16': '14.29',
            '5/8': '15.88',
            '11/16': '17.46',
            '3/4': '19.05',
            '7/8': '22.23',
            'spline': '—à–ª–∏—Ü–µ–≤–æ–π',
            '—à–ª–∏—Ü–µ–≤–æ–π': 'spline',
            'square': '–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π',
            '–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π': 'square',
            '–∫–≤–∞–¥—Ä–∞—Ç': 'square',
            'awg': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π-–∫–∞–ª–∏–±—Ä',
            'wire': '–ø—Ä–æ–≤–æ–¥',
            '–ø—Ä–æ–≤–æ–¥': 'wire',
            'cable': '–∫–∞–±–µ–ª—å',
            '–∫–∞–±–µ–ª—å': 'cable',
            'sds-max': 'sds-–º–∞–∫—Å',
            'sds-plus': 'sds-–ø–ª—é—Å',
            'drill': '—Å–≤–µ—Ä–ª–æ',
            '—Å–≤–µ—Ä–ª–æ': 'drill',
            '–±—É—Ä': 'drill',

            # –ù–û–í–´–ï –°–ò–ù–û–ù–ò–ú–´ –ò–ó –í–ê–®–ï–ì–û –§–ê–ô–õ–ê
            '–Ω–∞–∫–∞—á–∫–∞': '–ø–æ–¥–∫–∞—á–∫–∞',
            '–ø–æ–¥–∫–∞—á–∫–∞': '–Ω–∞–∫–∞—á–∫–∞',
            '–±–∏—Ç–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å': '–¥–µ—Ä–∂–∞—Ç–µ–ª—å',
            '–¥–µ—Ä–∂–∞—Ç–µ–ª—å': '–±–∏—Ç–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å',
            '–ø—Ä–æ–¥—É–≤–∫–∞': '–ø—Ä–æ–¥—É–≤–æ—á–Ω—ã–π',
            '–ø—Ä–æ–¥—É–≤–æ—á–Ω—ã–π': '–ø—Ä–æ–¥—É–≤–∫–∞',
            '–º–æ–Ω—Ç–∏—Ä–æ–≤–∫–∞': '–º–æ–Ω—Ç–∞–∂–∫–∞',
            '–º–æ–Ω—Ç–∞–∂–∫–∞': '–º–æ–Ω—Ç–∏—Ä–æ–≤–∫–∞',
            '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è': '–¥–∏—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è',
            '–¥–∏—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è': '–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è',
            '–æ–±–∂–∏–º–∞—Ç–µ–ª—å': '–æ–±–∂–∏–º–Ω—ã–µ',
            '–æ–±–∂–∏–º–Ω—ã–µ': '–æ–±–∂–∏–º–∞—Ç–µ–ª—å',
            '–≥–≤–æ–∑–¥–æ–¥–µ—Ä': '–≥–≤–æ–∑–¥–æ–¥–µ—Ä–æ–º',
            '–≥–≤–æ–∑–¥–æ–¥–µ—Ä–æ–º': '–≥–≤–æ–∑–¥–æ–¥–µ—Ä',
            '–ø–Ω–µ–≤–º–æ–≥–∞–π–∫–æ–≤–µ—Ä—Ç': '–≥–∞–π–∫–æ–≤–µ—Ä—Ç',
            '–≥–∞–π–∫–æ–≤–µ—Ä—Ç': '–ø–Ω–µ–≤–º–æ–≥–∞–π–∫–æ–≤–µ—Ä—Ç',
            '—Å–µ–ø–æ—Ä–∞—Ç–æ—Ä': '—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–æ–º',
            '—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–æ–º': '—Å–µ–ø–æ—Ä–∞—Ç–æ—Ä',
            '—Ç—Ä—É–±–æ—Ä–µ–∑-–Ω–æ–∂–Ω–∏—Ü—ã': '—Ç—Ä—É–±–æ—Ä–µ–∑',
            '—Ç—Ä—É–±–æ—Ä–µ–∑': '—Ç—Ä—É–±–æ—Ä–µ–∑-–Ω–æ–∂–Ω–∏—Ü—ã',
            '—Ç—Ä—ë—Ö–ª–∞–ø—ã–π': '—Ç—Ä–µ—Ö–∑–∞—Ö–≤–∞—Ç–Ω—ã–π',
            '—Ç—Ä–µ—Ö–∑–∞—Ö–≤–∞—Ç–Ω—ã–π': '—Ç—Ä—ë—Ö–ª–∞–ø—ã–π',
            '3-—Ö': '—Ç—Ä–µ—Ö–∑–∞—Ö–≤–∞—Ç–Ω—ã–π',
            '–±–æ–ª—Ç–æ—Ä–µ–∑': '–±–æ–ª—Ç–æ—Ä–µ–∑—ã',
            '–±–æ–ª—Ç–æ—Ä–µ–∑—ã': '–±–æ–ª—Ç–æ—Ä–µ–∑',
            '–¥–≤—É—Ö–ª–∞–ø—ã–π': '–¥–≤—É–∑–∞—Ö–≤–∞—Ç–Ω—ã–π',
            '–¥–≤—É–∑–∞—Ö–≤–∞—Ç–Ω—ã–π': '–¥–≤—É—Ö–ª–∞–ø—ã–π',
            '2-—Ö': '–¥–≤—É–∑–∞—Ö–≤–∞—Ç–Ω—ã–π',
            '—É—Å–∏–ª–µ–Ω–Ω–∞—è': '—Å–∏–ª–æ–≤–∞—è',
            '—Å–∏–ª–æ–≤–∞—è': '—É—Å–∏–ª–µ–Ω–Ω–∞—è',
            '—Ä–∏—Ö—Ç–æ–≤–æ—á–Ω–∞—è': '–∫—É–∑–æ–≤–Ω–∞—è',
            '–∫—É–∑–æ–≤–Ω–∞—è': '—Ä–∏—Ö—Ç–æ–≤–æ—á–Ω–∞—è',
            '–ª–æ–ø–∞—Ç–∫–∞': '–º–æ–Ω—Ç–∏—Ä–æ–≤–∫–∞',

            # === –£–î–ê–†–ù–´–ï –ì–û–õ–û–í–ö–ò ===
            '—É–¥–∞—Ä–Ω—ã–µ': '—É–¥–∞—Ä–Ω–∞—è',
            '—É–¥–∞—Ä–Ω–∞—è': '—É–¥–∞—Ä–Ω—ã–µ',
            '—É–¥–ª–∏–Ω–µ–Ω–Ω—ã—Ö': '—É–¥–ª–∏–Ω–µ–Ω–Ω—ã–µ',
            '—É–¥–ª–∏–Ω–µ–Ω–Ω—ã–µ': '—É–¥–ª–∏–Ω–µ–Ω–Ω—ã—Ö',
            '–≥–æ–ª–æ–≤–æ–∫ —É–¥–∞—Ä–Ω—ã—Ö': '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏',
            '—É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫': '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏',
            '—É–¥–∞—Ä–Ω—ã—Ö —É–¥–ª–∏–Ω–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫': '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏',
            '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö': '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏',
            '–Ω–∞–±–æ—Ä –≥–æ–ª–æ–≤–æ–∫ —É–¥–∞—Ä–Ω—ã—Ö': '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏',
            'impact': '—É–¥–∞—Ä–Ω—ã–µ',

            # === –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –î–õ–Ø –°–ù–Ø–¢–ò–Ø –ò–ó–û–õ–Ø–¶–ò–ò ===
            '—Å–Ω—è—Ç–∏—è –∏–∑–æ–ª—è—Ü–∏–∏': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–Ω—è—Ç–∏—è –∏–∑–æ–ª—è—Ü–∏–∏': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '—â–∏–ø—Ü—ã –¥–ª—è –∑–∞—á–∏—Å—Ç–∫–∏': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–∑–∞—á–∏—Å—Ç–∫–∏ —ç–ª–µ–∫—Ç—Ä–æ–ø—Ä–æ–≤–æ–¥–æ–≤': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–∑–∞—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–≤–æ–¥–æ–≤': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '—Å–Ω—è—Ç–∏—è –æ–±–æ–ª–æ—á–∫–∏': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–¥–∏—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π –¥–ª—è —Å–Ω—è—Ç–∏—è': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–∏—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–¥–ª—è —Å–Ω—è—Ç–∏—è': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–¥–ª—è –∑–∞—á–∏—Å—Ç–∫–∏': '—Å—Ç—Ä–∏–ø–ø–µ—Ä',
            '–∫–ª–µ—â–∏ —ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ': '–≤–∏—Ç–æ–π –ø–∞—Ä—ã',
            '–≤–∏—Ç–æ–π –ø–∞—Ä—ã': '–∫–ª–µ—â–∏ —ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ',
            '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ': '–∏–∑–æ–ª—è—Ü–∏–∏',
            '–∏–∑–æ–ª—è—Ü–∏–∏': '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ',

            # === –ü–ù–ï–í–ú–û–†–ê–ó–™–ï–ú–´ –ò –§–ò–¢–ò–ù–ì–ò ===
            '–ø–Ω–µ–≤–º–æ—Ä–∞–∑—ä—ë–º': '—Ñ–∏—Ç–∏–Ω–≥',
            '–ø–Ω–µ–≤–º–æ—Ä–∞–∑—ä–µ–º': '—Ñ–∏—Ç–∏–Ω–≥',
            '–±—ã—Å—Ç—Ä–æ—Å—ä—ë–º': '—Ñ–∏—Ç–∏–Ω–≥',
            '–±—ã—Å—Ç—Ä–æ—Å—ä–µ–º–Ω—ã–π': '—Ñ–∏—Ç–∏–Ω–≥',
            '–±—ã—Å—Ç—Ä–æ—Ä–∞–∑—ä–µ–º–Ω—ã–π': '—Ñ–∏—Ç–∏–Ω–≥',
            '—Ñ–∏—Ç–∏–Ω–≥ –ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫': '—Ñ–∏—Ç–∏–Ω–≥',
            '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫ –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π': '—Ñ–∏—Ç–∏–Ω–≥',
            '–ø–Ω–µ–≤–º–æ–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫': '—Ñ–∏—Ç–∏–Ω–≥',
            '–ü–Ω–µ–≤–º–æ—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞': '—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞',
            '—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞': '–ü–Ω–µ–≤–º–æ—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞',
            '—à–ª–∏—Ñ–æ–≤–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞': '–ü–Ω–µ–≤–º–æ—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞',
            '–ü–Ω–µ–≤–º–æ—à–ª–∏—Ñ–º–∞—à–∏–Ω–∞': '—à–ª–∏—Ñ–æ–≤–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞',
            '–¥—Ä–µ–ª—å –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∞—è': '–ü–Ω–µ–≤–º–æ–¥—Ä–µ–ª—å',
            '–ü–Ω–µ–≤–º–æ–¥—Ä–µ–ª—å': '–¥—Ä–µ–ª—å –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∞—è',


            # === –°–™–ï–ú–ù–ò–ö–ò –ü–û–î–®–ò–ü–ù–ò–ö–û–í ===
            '—Å—ä–µ–º–Ω–∏–∫ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä': '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π',
            '—Å—ä–µ–º–Ω–∏–∫ –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π': '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π',
            '—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π —Å—ä–µ–º–Ω–∏–∫': '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π',
            '—Å—ä–µ–º–Ω–∏–∫ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω–æ–≥–æ —Ç–∏–ø–∞': '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π',
            '—Ç—Ä–∏ –∑–∞—Ö–≤–∞—Ç–∞': '—Ç—Ä—ë—Ö–ª–∞–ø—ã–π',
            '—Ç—Ä—ë—Ö–ª–∞–ø—ã–π': '—Ç—Ä–∏ –∑–∞—Ö–≤–∞—Ç–∞',
            '–¥–≤–∞ –∑–∞—Ö–≤–∞—Ç–∞': '–¥–≤—É—Ö–ª–∞–ø—ã–π',
            '–¥–≤—É—Ö–ª–∞–ø—ã–π': '–¥–≤–∞ –∑–∞—Ö–≤–∞—Ç–∞',

            # === –¶–ê–ù–ì–ê ===
            '—Ü–∞–Ω–≥–∞': '—Ü–∞–Ω–≥–æ–≤—ã–π —Ä–∞–∑—ä–µ–º',
            '—Ü–∞–Ω–≥–æ–≤—ã–π —Ä–∞–∑—ä–µ–º': '—Ü–∞–Ω–≥–∞',
            '—Ü–∞–Ω–≥–æ–≤—ã–π': '—Ü–∞–Ω–≥–∞',

            # === –†–ê–ó–ù–û–ï ===
            '–∫–ª—é—á-–Ω–∞—Å–∞–¥–∫–∞': '–ù–∞—Å–∞–¥–∫–∞',
            '–ù–∞—Å–∞–¥–∫–∞': '–∫–ª—é—á-–Ω–∞—Å–∞–¥–∫–∞',
            '—Å–∫–æ–±–∞': '—Å–∫–æ–±—ã',

        }

        # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í
        self.tool_categories = {
            'wrenches': ['–∫–ª—é—á', '–≥–∞–µ—á–Ω—ã–π', '–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '—Ä–æ–∂–∫–æ–≤—ã–π', '–Ω–∞–∫–∏–¥–Ω–æ–π', '—Ä–∞–∑–≤–æ–¥–Ω–æ–π', '–≤–µ–Ω—Ç–∏–ª—å–Ω—ã–π'],
            'screwdrivers': ['–æ—Ç–≤–µ—Ä—Ç–∫–∞', '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è', '–±–∏—Ç–∞', '–±–∏—Ç—ã', 'phillips', 'slotted', 'torx', 'hex'],
            'impact_sockets': ['—É–¥–∞—Ä–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏', '—É–¥–∞—Ä–Ω–∞—è –≥–æ–ª–æ–≤–∫–∞', '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏', '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö', 'impact'],
            'sockets': ['–≥–æ–ª–æ–≤–∫–∞', '–Ω–∞—Å–∞–¥–∫–∞', '—Ç–æ—Ä—Ü–µ–≤–∞—è', '—Ç–æ—Ä—Ü–æ–≤–∞—è', 'socket', '—É–¥–∞—Ä–Ω—ã–µ'],
            'strippers': ['—Å—Ç—Ä–∏–ø–ø–µ—Ä', '—Å–Ω—è—Ç–∏—è –∏–∑–æ–ª—è—Ü–∏–∏', '–∑–∞—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–≤–æ–¥–æ–≤', '—â–∏–ø—Ü—ã –¥–ª—è –∑–∞—á–∏—Å—Ç–∫–∏', '–¥–ª—è —Å–Ω—è—Ç–∏—è –∏–∑–æ–ª—è—Ü–∏–∏', '–æ–±–∂–∏–º–∞—Ç–µ–ª—å', '–∫—Ä–∏–º–ø–µ—Ä', '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–Ω—Ç–∞–∂–Ω—ã–µ', '–æ–±–∂–∏–º–Ω—ã–µ'],
            'pliers': ['–ø–∞—Å—Å–∞—Ç–∏–∂–∏', '–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã', '–∫—É—Å–∞—á–∫–∏', '–±–æ–∫–æ—Ä–µ–∑—ã', '–∑–∞–∂–∏–º', '–∫–ª–µ—â–∏'],
            'hammers': ['–º–æ–ª–æ—Ç–æ–∫', '–∫—É–≤–∞–ª–¥–∞', '–∫–∏—è–Ω–∫–∞', 'hammer', '–≥–≤–æ–∑–¥–æ–¥–µ—Ä'],
            'files_rasps': ['–Ω–∞–ø–∏–ª—å–Ω–∏–∫', '—Ä–∞—à–ø–∏–ª—å', '–Ω–∞–¥—Ñ–∏–ª—å'],
            'scissors': ['–Ω–æ–∂–Ω–∏—Ü—ã', '–±–æ–ª—Ç–æ—Ä–µ–∑', '–±–æ–ª—Ç–æ—Ä–µ–∑—ã', '—Ç—Ä—É–±–æ—Ä–µ–∑'],
            'clamps': ['—Å—Ç—Ä—É–±—Ü–∏–Ω–∞', '—Ç–∏—Å–∫–∏', '–∑–∞–∂–∏–º', 'clamp', '–∑–∞—Ö–≤–∞—Ç'],
            'drills': ['—Å–≤–µ—Ä–ª–æ', '–±—É—Ä', 'drill', '–∫–æ—Ä–æ–Ω–∫–∞'],
            'cutting_tools': ['—Ä–µ–∑–µ—Ü', '—Ñ—Ä–µ–∑–∞', '–º–µ—Ç—á–∏–∫', '–ø–ª–∞—à–∫–∞'],
            'measuring': ['–º–∞–Ω–æ–º–µ—Ç—Ä', '–≤–∞–∫—É—É–º–º–µ—Ç—Ä', '–º–∞–Ω–æ–≤–∞–∫—É—É–º–º–µ—Ç—Ä', '—à—Ç–∞–Ω–≥–µ–Ω—Ü–∏—Ä–∫—É–ª—å'],
            'pneumatic': ['–ø–Ω–µ–≤–º–æ', '–ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '—Ç—Ä–µ—â–æ—Ç–∫–∞', '–∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä', '–≥–∞–π–∫–æ–≤–µ—Ä—Ç'],
            'pneumatic_accessories': ['—Ñ–∏—Ç–∏–Ω–≥', '—Ä–∞–∑–≤–µ—Ç–≤–∏—Ç–µ–ª—å'],
            'consumables': ['–ø–∞—Å—Ç–∞', '—Å–º–∞–∑–∫–∞', '–ª–µ–Ω—Ç–∞', '–∞–±—Ä–∞–∑–∏–≤', '–æ—á–∏—Å—Ç–∏—Ç–µ–ª—å'],
            'accessories': ['–¥–µ—Ä–∂–∞—Ç–µ–ª—å', '–±–∏—Ç–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å', '–ø–µ—Ä–µ—Ö–æ–¥–Ω–∏–∫', '–∞–¥–∞–ø—Ç–µ—Ä', '—É–¥–ª–∏–Ω–∏—Ç–µ–ª—å'],
            'sets': ['–Ω–∞–±–æ—Ä', '–∫–æ–º–ø–ª–µ–∫—Ç', '–∫–µ–π—Å', '–ª–æ–∂–µ–º–µ–Ω—Ç'],
            'mounting': ['–º–æ–Ω—Ç–∏—Ä–æ–≤–∫–∞', '–º–æ–Ω—Ç–∞–∂–∫–∞', '–ª–æ–º', '–ª–æ–ø–∞—Ç–∫–∞'],
            'other_tools': ['–∑–µ—Ä–∫–∞–ª–æ', '–±—Ä–µ–ª–æ–∫', '–Ω–æ–∂', '–ø–ª–∞—Å—Ç–∏–Ω–∞', '—à–ø—Ä–∏—Ü', '—Å–∫—Ä–µ–±–æ–∫'],
            'pneumatic_pumps': ['–Ω–∞–∫–∞—á–∫–∞', '–ø–æ–¥–∫–∞—á–∫–∞', '–ø—Ä–æ–¥—É–≤–∫–∞'],
            'pneumatic_drils': ['–¥—Ä–µ–ª—å –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∞—è', '–ø–Ω–µ–≤–º–æ–¥—Ä–µ–ª—å'],
            'pneumatic_fittings': ['—Ñ–∏—Ç–∏–Ω–≥', '—Ä–∞–∑–≤–µ—Ç–≤–∏—Ç–µ–ª—å', '—à—Ç—É—Ü–µ—Ä', '–º—É—Ñ—Ç–∞', '–ø–Ω–µ–≤–º–æ—Ä–∞–∑—ä–µ–º' ,'–±—ã—Å—Ç—Ä–æ—Å—ä–µ–º'],
            'pneumatic_hammers': ['–ø–Ω–µ–≤–º–æ–¥–æ–ª–æ—Ç–æ', '–º–æ–ª–æ—Ç–æ–∫ –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '–ø–Ω–µ–≤–º–æ–º–æ–ª–æ—Ç–æ–∫', '—Ç—Ä–∞–º–±–æ–≤–∫–∞ –ø–Ω–µ–≤–º–∞—Ç–∏—á–µ—Å–∫–∞—è', '–ø–Ω–µ–≤–º–æ—Ç—Ä–∞–º–±–æ–≤–∫–∞'],
            'pullers': ['—Å—ä–µ–º–Ω–∏–∫', '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π', '—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π' ,'—Å—ä–µ–º–Ω–∏–∫ –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤'],
            'hoist': ['—Ç–∞–ª—å', '—Ç–∞–ª–∏', '–ª–µ–±–µ–¥–∫–∞', '–ø–æ–¥—ä–µ–º–Ω–∏–∫', '–∫—Ä–∞–Ω-–±–∞–ª–∫–∞', '—Ç–µ–ª—å—Ñ–µ—Ä'],
            'cable cutter': ['–∫–∞–±–µ–ª–µ—Ä–µ–∑', '—Ç—Ä–æ—Å–æ—Ä–µ–∑', 'c–µ–∫—Ç–æ—Ä–Ω—ã–µ –Ω–æ–∂–Ω–∏—Ü—ã', '–∫–∞–±–µ–ª–µ—Ä–µ–∑ —Å–µ–∫—Ç–æ—Ä–Ω—ã–π', '–Ω–æ–∂–Ω–∏—Ü—ã —Å–µ–∫—Ç–æ—Ä–Ω—ã–µ'],
            'staples': ['—Å–∫–æ–±–∞', '—Å–∫–æ–±—ã'],
        }

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ù–ï–°–û–í–ú–ï–°–¢–ò–ú–´–ï –ü–ê–†–´ (–∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞)
        self.incompatible_pairs = [
            (['sockets'], ['sets']),  # –≥–æ–ª–æ–≤–∫–∞ –Ω–µ —Ä–∞–≤–Ω–æ –Ω–∞–±–æ—Ä
            (['sockets'], ['wrenches']),  # –≥–æ–ª–æ–≤–∫–∏ –Ω–µ —Ä–∞–≤–Ω–æ –∫–ª—é—á–∏
            (['consumables'], ['clamps']),  # –ø–∞—Å—Ç–∞ –Ω–µ —Ä–∞–≤–Ω–æ —Ç–∏—Å–∫–∏
            (['screwdrivers'], ['drills']),  # –æ—Ç–≤–µ—Ä—Ç–∫–∞ –Ω–µ —Ä–∞–≤–Ω–æ —Å–≤–µ—Ä–ª–æ
            (['accessories'], ['sets']),  # –¥–µ—Ä–∂–∞—Ç–µ–ª—å –Ω–µ —Ä–∞–≤–Ω–æ –Ω–∞–±–æ—Ä
            (['scissors'], ['consumables']),  # –Ω–æ–∂–Ω–∏—Ü—ã –Ω–µ —Ä–∞–≤–Ω–æ –ª–µ–Ω—Ç–∞
            (['files_rasps'], ['pneumatic_accessories']),  # –Ω–∞–ø–∏–ª—å–Ω–∏–∫ –Ω–µ —Ä–∞–≤–Ω–æ —à–ª–∞–Ω–≥
            (['measuring'], ['hammers']),  # –º–∞–Ω–æ–º–µ—Ç—Ä –Ω–µ —Ä–∞–≤–Ω–æ –º–æ–ª–æ—Ç–æ–∫
            (['wrenches'], ['accessories']),  # –∫–ª—é—á –Ω–µ —Ä–∞–≤–Ω–æ –Ω–∞—Å–∞–¥–∫–∞
            (['screwdrivers'], ['pneumatic']),  # –æ—Ç–≤–µ—Ä—Ç–∫–∞ –Ω–µ —Ä–∞–≤–Ω–æ –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä
            (['other_tools'], ['pneumatic_accessories']),  # –∑–µ—Ä–∫–∞–ª–æ –Ω–µ —Ä–∞–≤–Ω–æ —Ñ–∏—Ç–∏–Ω–≥
            (['sockets'], ['cutting_tools']),  # –≥–æ–ª–æ–≤–∫–∞ –Ω–µ —Ä–∞–≤–Ω–æ —Ñ—Ä–µ–∑–∞
            (['hammers'], ['hoist']),  # –≥–æ–ª–æ–≤–∫–∞ –Ω–µ —Ä–∞–≤–Ω–æ —Ñ—Ä–µ–∑–∞
        ]

        # –í–ê–® –°–ü–ò–°–û–ö –ë–†–ï–ù–î–û–í (—Ä–∞—Å—à–∏—Ä–µ–Ω)
        self.brands = {
            'kingtony', 'king-tony', 'jtc', 'matrix', 'gross', 'stayer',
            'fit', 'kraftool', '–∑—É–±—Ä', '–≤–∏—Ö—Ä—å', '–ø–∞—Ç—Ä–∏–æ—Ç', '–∫–∞–ª–∏–±—Ä',
            '–∫—Ä–∞—Ç–æ–Ω', '–∏–Ω—Ç–µ—Ä—Å–∫–æ–ª', 'skrab', 'rockforce', 'forsage',
            'berger', 'hans', 'stanley', 'bosch', 'makita', 'sparta',
            'pressol', '–ø—Ä–µ—Å—Å–æ–ª'
        }

        self.whitelist = set()
        self._load_whitelist()

    def _load_whitelist(self, filepath='whitelist.xlsx'):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞ –ø–∞—Ä –∏–∑ Excel —Ñ–∞–π–ª–∞.

        –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç Excel:
        - –°—Ç–æ–ª–±–µ—Ü 1: –ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è
        - –°—Ç–æ–ª–±–µ—Ü 2: –ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞
        - –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è
        """
        self.whitelist = set()

        try:
            # –ß–∏—Ç–∞–µ–º Excel
            df = pd.read_excel(filepath)

            print(f"üìã –ß—Ç–µ–Ω–∏–µ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∏–∑ {filepath}...")

            if len(df.columns) < 2:
                print(f"‚ö†Ô∏è –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å—Ç–æ–ª–±—Ü–∞!")
                return

            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2 —Å—Ç–æ–ª–±—Ü–∞
            df = df.iloc[:, :2]
            df.columns = ['master', 'nomenclature']

            # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            df = df.dropna(subset=['master', 'nomenclature'])

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ whitelist
            for _, row in df.iterrows():
                master = str(row['master']).strip().lower()
                nomenclature = str(row['nomenclature']).strip().lower()

                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
                self.whitelist.add((master, nomenclature))
                self.whitelist.add((nomenclature, master))

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–∞—Ä –∏–∑ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞: {len(self.whitelist) // 2}")

        except FileNotFoundError:
            print(f"‚ö†Ô∏è –§–∞–π–ª –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞ '{filepath}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞: {e}")

    def _is_whitelisted(self, master, nomenclature):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–∞—Ä–∞ –≤ –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ"""
        master_normalized = master.strip().lower()
        nomenclature_normalized = nomenclature.strip().lower()

        return (master_normalized, nomenclature_normalized) in self.whitelist

    def normalize_technical_designations(self, text):
        """–í–ê–®–ê –§–£–ù–ö–¶–ò–Ø (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!)"""
        text = text.lower()
        text = re.sub(r'ph\s*(\d+)', r'phillips \1', text)
        text = re.sub(r'sl\s*(\d+(?:[.,]\d+)?)', r'slotted \1', text)
        text = re.sub(r't(\d+)', r'torx \1', text)
        text = re.sub(r'h(\d+)', r'hex \1', text)
        text = re.sub(r'pz\s*(\d+)', r'pozidriv \1', text)
        text = re.sub(r'g-?\s*(\d+)', r'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ \1', text)
        text = re.sub(r'f-?\s*(\d+)', r'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞ \1', text)

        # –£–¥–∞—Ä–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏
        text = re.sub(r'–Ω–∞–±–æ—Ä\s+(?:–≥–æ–ª–æ–≤–æ–∫\s+)?—É–¥–∞—Ä–Ω—ã—Ö(?:\s+—É–¥–ª–∏–Ω–µ–Ω–Ω—ã—Ö)?(?:\s+–≥–æ–ª–æ–≤–æ–∫)?', '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏', text)
        text = re.sub(r'—É–¥–∞—Ä–Ω—ã–µ\s+—É–¥–ª–∏–Ω–µ–Ω–Ω—ã–µ\s+–≥–æ–ª–æ–≤–∫–∏', '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏', text)
        text = re.sub(r'–≥–æ–ª–æ–≤–∫–∏\s+—É–¥–∞—Ä–Ω—ã–µ', '—É–¥–∞—Ä–Ω—ã–µ-–≥–æ–ª–æ–≤–∫–∏', text)

        # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–Ω—è—Ç–∏—è –∏–∑–æ–ª—è—Ü–∏–∏
        text = re.sub(r'–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\s+(?:–¥–∏—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π\s+)?–¥–ª—è\s+—Å–Ω—è—Ç–∏—è\s+(?:–∏–∑–æ–ª—è—Ü–∏–∏|–æ–±–æ–ª–æ—á–∫–∏)', '—Å—Ç—Ä–∏–ø–ø–µ—Ä', text)
        text = re.sub(r'—â–∏–ø—Ü—ã\s+–¥–ª—è\s+–∑–∞—á–∏—Å—Ç–∫–∏(?:\s+—ç–ª–µ–∫—Ç—Ä–æ)?–ø—Ä–æ–≤–æ–¥–æ–≤', '—Å—Ç—Ä–∏–ø–ø–µ—Ä', text)

        # –ü–Ω–µ–≤–º–æ—Ä–∞–∑—ä–µ–º—ã
        text = re.sub(r'–ø–Ω–µ–≤–º–æ—Ä–∞–∑—ä–µ[–º—ë]', '—Ñ–∏—Ç–∏–Ω–≥', text)
        text = re.sub(r'–±—ã—Å—Ç—Ä–æ(?:—Å—ä–µ|—Ä–∞–∑—ä–µ)[–º—ë]', '—Ñ–∏—Ç–∏–Ω–≥', text)

        # –°—ä–µ–º–Ω–∏–∫–∏ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–µ
        text = re.sub(r'—Å—ä–µ–º–Ω–∏–∫\s+(?:–ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤\s+)?—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω(?:—ã–π|–æ–≥–æ\s+—Ç–∏–ø–∞)', '—Å—ä–µ–º–Ω–∏–∫-—Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–Ω—ã–π', text)

        return text

    def _detect_tool_category(self, text):
        """–ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        text_lower = text.lower()
        detected_categories = []

        for category, keywords in self.tool_categories.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected_categories.append(category)
                    break

            #–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞, —á—Ç–æ–±—ã —Å—Ç–∞–ª—å –Ω–µ —Ä–∞–≤–Ω–æ —Ç–∞–ª—å
            # \b = –≥—Ä–∞–Ω–∏—Ü–∞ —Å–ª–æ–≤–∞ (–ø—Ä–æ–±–µ–ª, –∑–Ω–∞–∫ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü)
        # for category, keywords in self.tool_categories.items():
        #     for keyword in keywords:
        #         pattern = r'\b' + re.escape(keyword) + r'\b'
        #
        #         if re.search(pattern, text_lower):
        #             detected_categories.append(category)
        #             break

        return detected_categories if detected_categories else ['unknown']

    def _check_category_compatibility(self, master_categories, nomenclature_categories):
        """–ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        if 'unknown' in master_categories or 'unknown' in nomenclature_categories:
            return 0.4

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–∞—Ä—ã
        for incompatible_group in self.incompatible_pairs:
            group1, group2 = incompatible_group
            master_in_group1 = any(cat in group1 for cat in master_categories)
            nomenclature_in_group2 = any(cat in group2 for cat in nomenclature_categories)

            if master_in_group1 and nomenclature_in_group2:
                return 0.0  # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ê–ù–û–ú–ê–õ–ò–Ø!

        # –ü—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if set(master_categories) & set(nomenclature_categories):
            return 1.0

        return 0.3

    def analyze_similarity_batch(self, masters, nomenclatures):
        """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ê–®–ê –§–£–ù–ö–¶–ò–Ø"""
        print("üöÄ –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø UltraEnhancedTextAnalyzer —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–µ–π!")
        similarities = []

        for i, (master, nomenclature) in enumerate(zip(masters, nomenclatures)):
            master_norm = self.normalize_technical_designations(master)
            nomenclature_norm = self.normalize_technical_designations(nomenclature)

            # –ù–û–í–û–ï: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ
            semantic_score = self._enhanced_semantic_similarity(master_norm, nomenclature_norm)
            size_score = self._enhanced_size_similarity(master, nomenclature)

            # 95% —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (–≤–∫–ª—é—á–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏) + 5% —Ä–∞–∑–º–µ—Ä—ã
            total_score = semantic_score * 0.95 + size_score * 0.05
            similarities.append(min(1.0, max(0.0, total_score)))

            if i % 1000 == 0 and i > 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(masters)} –ø–∞—Ä")

        return similarities


    def _enhanced_semantic_similarity(self, master, nomenclature):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –º–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏–µ–π –∏ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–æ–π.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0.0 (–ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–Ω—ã–µ) –¥–æ 1.0 (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ).
        """

        # ========================================================================
        # –®–ê–ì 0: –ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –ò–î–ï–ù–¢–ò–ß–ù–û–°–¢–¨
        # ========================================================================
        master_normalized = master.lower().strip()
        nomenclature_normalized = nomenclature.lower().strip()

        # –ü–æ–ª–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å
        if master_normalized == nomenclature_normalized:
            return 1.0

        # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–æ —Å–ª–æ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        master_words = set(master_normalized.split())
        nom_words = set(nomenclature_normalized.split())

        quick_jaccard = 0.0  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

        if len(master_words) > 0 and len(nom_words) > 0:
            intersection = master_words & nom_words
            union = master_words | nom_words
            quick_jaccard = len(intersection) / len(union)

            # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å (90%+)
            if quick_jaccard >= 0.90:
                return 0.95

            # –í—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å (80%+)
            elif quick_jaccard >= 0.80:
                return 0.85

        # –®–ê–ì 0.5: –ü–†–û–í–ï–†–ö–ê FALSE POSITIVES –í –ü–†–ò–û–†–ò–¢–ï–¢–ï!
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
        if self._is_false_positive_impact_sockets(master, nomenclature):
            return 0.85

        if self._is_false_positive_wrench_attachments(master, nomenclature):
            return 0.85

        # ========================================================================
        # –®–ê–ì 1: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô
        # ========================================================================
        master_categories = self._detect_tool_category(master)
        nomenclature_categories = self._detect_tool_category(nomenclature)

        # ========================================================================
        # –®–ê–ì 2: –ü–†–û–í–ï–†–ö–ê –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –ö–ê–¢–ï–ì–û–†–ò–ô
        # ========================================================================
        category_compatibility = self._check_category_compatibility(
            master_categories,
            nomenclature_categories
        )

        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ = –∞–Ω–æ–º–∞–ª–∏—è
        if category_compatibility <= 0.1:
            return 0.05

        # ========================================================================
        # –®–ê–ì 3: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–†–ú–ò–ù–û–í
        # ========================================================================
        master_terms = self._extract_enhanced_terms(master)
        nomenclature_terms = self._extract_enhanced_terms(nomenclature)

        # ========================================================================
        # –®–ê–ì 4: –û–ë–†–ê–ë–û–¢–ö–ê –ü–£–°–¢–´–• –¢–ï–†–ú–ò–ù–û–í
        # ========================================================================
        if not master_terms or not nomenclature_terms:
            # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±—ã–ª–∏ –ø–æ—Ö–æ–∂–∏
            if quick_jaccard >= 0.70:
                return 0.80
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            return category_compatibility * 0.4

        # ========================================================================
        # –®–ê–ì 5: –ö–û–≠–§–§–ò–¶–ò–ï–ù–¢ –ñ–ê–ö–ö–ê–†–ê –ü–û –¢–ï–†–ú–ò–ù–ê–ú
        # ========================================================================
        intersection = len(master_terms & nomenclature_terms)
        union = len(master_terms | nomenclature_terms)
        base_similarity = intersection / union if union > 0 else 0.0

        # ========================================================================
        # –®–ê–ì 6: –í–ó–í–ï–®–ï–ù–ù–ê–Ø –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–•–û–ñ–ï–°–¢–¨
        # ========================================================================
        # 60% —Ç–µ—Ä–º–∏–Ω—ã + 40% –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ 80/20)
        final_similarity = (base_similarity * 0.6 + category_compatibility * 0.4)

        # ========================================================================
        # –®–ê–ì 7: –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê
        # ========================================================================
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
        if self._is_false_positive_impact_sockets(master, nomenclature):
            final_similarity = max(final_similarity, 0.85)

        # # –ü—Ä–∞–≤–∏–ª–æ 2: –ö–ª—é—á–∏-–Ω–∞—Å–∞–¥–∫–∏
        if self._is_false_positive_wrench_attachments(master, nomenclature):
            final_similarity = max(final_similarity, 0.85)

        # ========================================================================
        # –®–ê–ì 8: –§–ò–ù–ê–õ–¨–ù–ê–Ø –ó–ê–©–ò–¢–ê
        # ========================================================================
        if final_similarity is None:
            return 0.0

        if "–∫–∞–±–µ–ª–µ—Ä–µ–∑" in master.lower() and "—Ç–∞–ª—å" in nomenclature.lower():
            print(f"\nüîç DEBUG –º–æ–ª–æ—Ç–æ–∫ vs —Ç–∞–ª—å:")
            print(f"  master_categories: {master_categories}")
            print(f"  nomenclature_categories: {nomenclature_categories}")
            print(f"  category_compatibility: {category_compatibility}")
            print(f"  master_terms: {master_terms}")
            print(f"  nomenclature_terms: {nomenclature_terms}")
            print(f"  intersection: {master_terms & nomenclature_terms}")
            print(f"  base_similarity: {base_similarity}")
            print(f"  final_similarity: {final_similarity}")

        return final_similarity

    # def _enhanced_semantic_similarity(self, master, nomenclature):
    #     """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ê–®–ê –§–£–ù–ö–¶–ò–Ø —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
    #     # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –°–ù–ê–ß–ê–õ–ê
    #     master_categories = self._detect_tool_category(master)
    #     nomenclature_categories = self._detect_tool_category(nomenclature)
    #     category_compatibility = self._check_category_compatibility(master_categories, nomenclature_categories)
    #
    #     # 2. –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã - —Å—Ä–∞–∑—É –∞–Ω–æ–º–∞–ª–∏—è
    #     if category_compatibility <= 0.1:
    #         return 0.05
    #
    #     # 3. –í–ê–®–ê –ª–æ–≥–∏–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
    #     master_terms = self._extract_enhanced_terms(master)
    #     nomenclature_terms = self._extract_enhanced_terms(nomenclature)
    #
    #     if not master_terms or not nomenclature_terms:
    #         return category_compatibility * 0.4
    #
    #     # 4. –í–ê–®–ê –ª–æ–≥–∏–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    #     intersection = len(master_terms & nomenclature_terms)
    #     union = len(master_terms | nomenclature_terms)
    #     base_similarity = intersection / union if union > 0 else 0.0
    #
    #     # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
    #     final_similarity = (base_similarity * 0.6 + category_compatibility * 0.4)
    #
    #     # –ò—Å–∫–ª—é—á–∞–µ–º –ª–æ–∂–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ –¥–ª—è —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫
    #     if self._is_false_positive_impact_sockets(master, nomenclature):
    #         final_similarity = max(final_similarity, 0.85)
    #
    #     return final_similarity

        # # 5. –£–ë–ò–†–ê–ï–ú –∏–∑–±—ã—Ç–æ—á–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        # return (base_similarity * 0.6 + category_compatibility * 0.4)

    def _calculate_functional_bonus(self, master_terms, nomenclature_terms):
        """–í–ê–®–ê –§–£–ù–ö–¶–ò–Ø (—É–º–µ–Ω—å—à–µ–Ω –±–æ–Ω—É—Å)"""
        bonus = 0.0
        functional_groups = [
            {'phillips', '–∫—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è', 'ph'},
            {'slotted', '—à–ª–∏—Ü–µ–≤–∞—è', 'sl', '–ø—Ä—è–º–∞—è'},
            {'torx', '—Ç–æ—Ä–∫—Å', '–∑–≤–µ–∑–¥–æ—á–∫–∞'},
            {'hex', '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫', '–∏–º–±—É—Å'},
            {'g-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞', 'g-–æ–±—Ä–∞–∑–Ω–∞—è', '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞'},
            {'f-—Å—Ç—Ä—É–±—Ü–∏–Ω–∞', 'f-–æ–±—Ä–∞–∑–Ω–∞—è', '—Å—Ç—Ä—É–±—Ü–∏–Ω–∞'},
            {'–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '—Ä–æ–∂–∫–æ–≤–æ-–Ω–∞–∫–∏–¥–Ω–æ–π'},
            {'–æ—Ç–≤–µ—Ä—Ç–∫–∞', '–æ—Ç–≤–µ—Ä—Ç–æ—á–Ω–∞—è', '–±–∏—Ç–∞'},
            {'–º–æ–ª–æ—Ç–æ–∫', '–∫—É–≤–∞–ª–¥–∞', '–∫–∏—è–Ω–∫–∞'},
            {'–ø–ª–æ—Å–∫–æ–≥—É–±—Ü—ã', '–ø–∞—Å—Å–∞—Ç–∏–∂–∏', '–∫—É—Å–∞—á–∫–∏'}
        ]

        for group in functional_groups:
            master_has = bool(master_terms & group)
            nomenclature_has = bool(nomenclature_terms & group)
            if master_has and nomenclature_has:
                bonus += 0.2  # —É–º–µ–Ω—å—à–µ–Ω —Å 0.5 –¥–æ 0.2

        return bonus

    def _extract_enhanced_terms(self, text):
        """–í–ê–®–ê –§–£–ù–ö–¶–ò–Ø (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!)"""
        for old, new in self.technical_synonyms.items():
            text = text.replace(old, new)

        for brand in self.brands:
            text = re.sub(r'\b' + re.escape(brand) + r'\b', '', text)

        text = re.sub(r'\b\d+[a-z]+-?\d+[a-z]*\b', '', text)
        text = re.sub(r'\b[a-z]+-?\d+[a-z]*\b', '', text)
        text = re.sub(r'[^a-z–∞-—è—ë0-9\s-]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        terms = set()
        words = text.split()
        for word in words:
            if len(word) > 2 and word not in {'–¥–ª—è', '–∏–ª–∏', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '–º–º', '—Å–º'}:
                terms.add(word)

        return terms

    def _enhanced_size_similarity(self, master, nomenclature):
        """–í–ê–®–ê –§–£–ù–ö–¶–ò–Ø (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!)"""
        master_sizes = self._extract_sizes(master)
        nomenclature_sizes = self._extract_sizes(nomenclature)

        if not master_sizes or not nomenclature_sizes:
            return 0.5

        for m_size in master_sizes:
            for n_size in nomenclature_sizes:
                if abs(m_size - n_size) / max(m_size, n_size) <= 0.1:
                    return 1.0

        return 0.0

    def _extract_sizes(self, text):
        """–í–ê–®–ê –§–£–ù–ö–¶–ò–Ø (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!)"""
        sizes = []

        mm_matches = re.findall(r'(\d+(?:[.,]\d+)?)\s*–º–º', text.lower())
        for match in mm_matches:
            try:
                sizes.append(float(match.replace(',', '.')))
            except ValueError:
                continue

        inch_patterns = [r'(\d+/\d+)']
        for pattern in inch_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    parts = match.split('/')
                    if len(parts) == 2:
                        inch_size = float(parts[0]) / float(parts[1])
                        sizes.append(inch_size * 25.4)
                except (ValueError, ZeroDivisionError):
                    continue

        numbers = re.findall(r'\b(\d+)\b', text)
        for num in numbers:
            try:
                n = float(num)
                if 4 <= n <= 50:
                    sizes.append(n)
            except ValueError:
                continue

        return sizes

    import re

    def _is_false_positive_impact_sockets(self, master, nomenclature):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫"""
        master_lower = master.lower()
        nomenclature_lower = nomenclature.lower()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫
        impact_patterns = [
            '—É–¥–∞—Ä–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏',
            '—É–¥–∞—Ä–Ω–∞—è –≥–æ–ª–æ–≤–∫–∞',
            '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö',
            '–Ω–∞–±–æ—Ä –≥–æ–ª–æ–≤–æ–∫ —É–¥–∞—Ä–Ω—ã—Ö',
            '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫',
            '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö –≥–ª—É–±–æ–∫–∏—Ö',
            '–Ω–∞–±–æ—Ä —É–¥–∞—Ä–Ω—ã—Ö –≥–ª—É–±–æ–∫–∏—Ö —Ç–æ—Ä—Ü–µ–≤—ã—Ö –≥–æ–ª–æ–≤–æ–∫',
            '—É–¥–∞—Ä–Ω—ã—Ö —É–¥–ª–∏–Ω–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫',
            '–≥–æ–ª–æ–≤–æ–∫ —É–¥–∞—Ä–Ω—ã—Ö',
            '—É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫'
        ]

        # –ï—Å–ª–∏ –≤ –æ–±–µ–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —É–¥–∞—Ä–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫ - —ç—Ç–æ –Ω–µ –∞–Ω–æ–º–∞–ª–∏—è
        master_has_impact = any(pattern in master_lower for pattern in impact_patterns)
        nomenclature_has_impact = any(pattern in nomenclature_lower for pattern in impact_patterns)

        return master_has_impact and nomenclature_has_impact

    def _is_false_positive_wrench_attachments(self, master, nomenclature):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è –Ω–∞—Å–∞–¥–æ–∫"""
        master_lower = re.sub(r'\s+', ' ', master.lower())
        nomenclature_lower = re.sub(r'\s+', ' ', nomenclature.lower())

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ (–≤–∫–ª—é—á–∞—è —Ç—Ä–µ—â–æ—Ç–æ—á–Ω—ã–µ!)
        attachment_types = [
            '–Ω–∞–∫–∏–¥–Ω–∞—è', '–Ω–∞–∫–∏–¥–Ω–æ–π', '–Ω–∞–∫–∏–¥–Ω—ã–µ',
            '—Ä–æ–∂–∫–æ–≤–∞—è', '—Ä–æ–∂–∫–æ–≤—ã–π', '—Ä–æ–∂–∫–æ–≤—ã–µ',
            '—Ç–æ—Ä—Ü–µ–≤–∞—è', '—Ç–æ—Ä—Ü–µ–≤–æ–π', '—Ç–æ—Ä—Ü–µ–≤—ã–µ',
            '—Ä–∞–∑—Ä–µ–∑–Ω–∞—è', '—Ä–∞–∑—Ä–µ–∑–Ω–æ–π', '—Ä–∞–∑—Ä–µ–∑–Ω—ã–µ',
            '—Ç—Ä–µ—â–æ—Ç–æ—á–Ω–∞—è', '—Ç—Ä–µ—â–æ—Ç–æ—á–Ω—ã–π', '—Ç—Ä–µ—â–æ—Ç–æ—á–Ω—ã–µ', '—Ç—Ä–µ—â–æ—Ç'  # –î–û–ë–ê–í–õ–ï–ù–û
        ]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –û–±–∞ –æ–ø–∏—Å—ã–≤–∞—é—Ç –Ω–∞—Å–∞–¥–∫–∏ –¥–ª—è –∫–ª—é—á–µ–π
        key_terms_master = (
                '–∫–ª—é—á-–Ω–∞—Å–∞–¥–∫–∞' in master_lower or
                ('–∫–ª—é—á' in master_lower and '–Ω–∞—Å–∞–¥–∫–∞' in master_lower) or
                '–ø—Ä–∏–≤–æ–¥–Ω–æ–π –∫–≤–∞–¥—Ä–∞—Ç' in master_lower or
                '—Ö–≤–æ—Å—Ç–æ–≤–∏–∫' in master_lower
        )

        key_terms_nomenclature = (
                '–∫–ª—é—á-–Ω–∞—Å–∞–¥–∫–∞' in nomenclature_lower or
                ('–Ω–∞—Å–∞–¥–∫–∞' in nomenclature_lower and '–∫–ª—é—á' in nomenclature_lower) or
                '–¥–∏–Ω–∞–º–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫' in nomenclature_lower or
                '–∫–≤–∞–¥—Ä–∞—Ç' in nomenclature_lower
        )

        if not (key_terms_master and key_terms_nomenclature):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ (–±–æ–ª–µ–µ –≥–∏–±–∫–∞—è)
        master_types = [t for t in attachment_types if t in master_lower]
        nomenclature_types = [t for t in attachment_types if t in nomenclature_lower]

        # –ï—Å–ª–∏ –æ–±–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–∏–ø—ã - –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å
        if master_types and nomenclature_types:
            if not set(master_types) & set(nomenclature_types):
                return False

        # –ï—Å–ª–∏ —Ç–∏–ø–æ–≤ –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å "–Ω–∞—Å–∞–¥–∫–∞" + "–∫–ª—é—á" - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
        def extract_all_numbers(text):
            """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —á–∏—Å–ª–∞ –≤–∫–ª—é—á–∞—è –¥—é–π–º–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã"""
            numbers = set()

            # –û–±—ã—á–Ω—ã–µ —á–∏—Å–ª–∞
            text_normalized = re.sub(r'(\d+)\s*[x*—ÖX√ó-]\s*(\d+)', r'\1 \2', text)
            numbers.update(re.findall(r'\b\d+\b', text_normalized))

            # –î—é–π–º–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã: 1/2", 3/8", 1/4"
            inch_sizes = re.findall(r'(\d+)/(\d+)', text)
            for numerator, denominator in inch_sizes:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É "1/2" –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                numbers.add(f"{numerator}/{denominator}")

            return numbers

        master_numbers = extract_all_numbers(master_lower)
        nomenclature_numbers = extract_all_numbers(nomenclature_lower)

        common_numbers = master_numbers & nomenclature_numbers

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –ë–æ–ª–µ–µ –≥–∏–±–∫–æ–µ —É—Å–ª–æ–≤–∏–µ –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
        # –î–ª—è —Ç—Ä–µ—â–æ—Ç–æ—á–Ω—ã—Ö - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 1 –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–æ–±—ã—á–Ω–æ –¥—é–π–º–æ–≤—ã–π)
        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –º–∏–Ω–∏–º—É–º 1 –∑–Ω–∞—á–∏–º–æ–µ —á–∏—Å–ª–æ (–Ω–µ –∞—Ä—Ç–∏–∫—É–ª)
        if len(common_numbers) >= 1:
            # –£–±–∏—Ä–∞–µ–º —è–≤–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã (4-5 –∑–Ω–∞—á–Ω—ã–µ —á–∏—Å–ª–∞)
            significant_numbers = {n for n in common_numbers if len(str(n)) <= 3 or '/' in str(n)}
            if significant_numbers:
                return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 5: –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π - –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –±–ª–∏–∑–∫–∏ (¬±1)
        # –ù–∞–ø—Ä–∏–º–µ—Ä, "14x18" –∏ "14—Ö18" —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏—è–º–∏
        if len(common_numbers) >= 2:
            return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 6: –ï—Å–ª–∏ –≤ –æ–±–æ–∏—Ö –µ—Å—Ç—å "–ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç–æ–≤–∏–∫" –∏–ª–∏ "–ø–æ—Å–∞–¥–æ—á–Ω—ã–π –∫–≤–∞–¥—Ä–∞—Ç"
        shaft_keywords = ['—Ö–≤–æ—Å—Ç–æ–≤–∏–∫', '–∫–≤–∞–¥—Ä–∞—Ç', '–ø–æ—Å–∞–¥–æ—á–Ω']
        master_has_shaft = any(kw in master_lower for kw in shaft_keywords)
        nomenclature_has_shaft = any(kw in nomenclature_lower for kw in shaft_keywords)

        if master_has_shaft and nomenclature_has_shaft and len(common_numbers) >= 1:
            return True

        return False

    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    # _is_false_positive_wrench_attachments(master="–∫–ª—é—á-–Ω–∞—Å–∞–¥–∫–∞ —Ä–æ–∂–∫–æ–≤–∞—è –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç–æ–≤–∏–∫ 9x12 –º–º 15 –º–º",nomenclature= "–ù–∞—Å–∞–¥–∫–∞ –¥–ª—è –¥–∏–Ω–∞–º–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–ª—é—á–∞ —Ä–æ–∂–∫–æ–≤–∞—è 15 –º–º, —Å –ø–æ—Å–∞–¥–æ—á–Ω—ã–º –∫–≤–∞–¥—Ä–∞—Ç–æ–º 9*12")


class FileReader:
    def read_data(self, file_path):
        print(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}")
        if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            df = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')
        else:
            df = pd.read_csv(file_path, encoding='utf-8')

        if len(df.columns) >= 2:
            df.columns = ['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è', '–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'] + list(df.columns[2:])

        df = df.dropna(subset=['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è', '–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'])
        df['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'] = df['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'].astype(str)
        df['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'] = df['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'].astype(str)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
        return df


class FileWriter:
    def save_anomalies(self, df, file_path):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        df_with_meta = df.copy()
        df_with_meta['detection_timestamp'] = pd.Timestamp.now()
        df_with_meta.to_csv(file_path, index=False, encoding='utf-8')
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {file_path}")
        return file_path


def main():
    logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')

    possible_files = [
        'data/input.xlsx',
        'data/master_nomenclature.xlsx',
        '–ú–∞—Å—Ç–µ—Ä –ø–æ–∑–∏—Ü–∏—è - –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞ ‚Äî –∫–æ–ø–∏—è.xlsx',
        '../data/input.xlsx'
    ]

    input_file = None
    for path in possible_files:
        if os.path.exists(path):
            input_file = path
            break

    if not input_file:
        print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    reader = FileReader()
    writer = FileWriter()
    analyzer = UltraEnhancedTextAnalyzer()

    df = reader.read_data(input_file)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

    original_count = len(df)

    whitelist_mask = df.apply(
        lambda row: analyzer._is_whitelisted(
            row['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'],
            row['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞']
        ),
        axis=1
    )

    whitelisted_count = whitelist_mask.sum()

    if whitelisted_count > 0:
        print(f"‚úÖ –ü–∞—Ä –≤ –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ: {whitelisted_count}")
        whitelisted_df = df[whitelist_mask].copy()
        writer.save_anomalies(whitelisted_df, 'data/whitelisted_pairs.csv')
        print(f"üìÑ –ü—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    df = df[~whitelist_mask].copy()
    print(f"üìä –ü–∞—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(df)}")

    if len(df) == 0:
        print("‚ö†Ô∏è –í—Å–µ –ø–∞—Ä—ã –≤ –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ.")
        return
    # ========================================================================

    print("üîç –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
    similarities = analyzer.analyze_similarity_batch(
        df['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'].tolist(),
        df['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'].tolist()
    )

    df['similarity_score'] = similarities
    threshold = 0.1771

    print(f"üéØ –í—ã—á–∏—Å–ª–µ–Ω –ø–æ—Ä–æ–≥ –∞–Ω–æ–º–∞–ª–∏–π: {threshold:.3f}")

    df['is_anomaly'] = df['similarity_score'] < threshold
    anomalies = df[df['is_anomaly']].copy()

    def should_exclude_impact_sockets(master, nomenclature):
        m, n = master.lower(), nomenclature.lower()
        if not ('—É–¥–∞—Ä–Ω' in m and '—É–¥–∞—Ä–Ω' in n):
            return False
        keywords = ['–≥–æ–ª–æ–≤–∫', '–Ω–∞–±–æ—Ä', '–∫–æ–º–ø–ª–µ–∫—Ç', 'dr', '1/2', '3/4', '1/4']
        return any(k in m for k in keywords) and any(k in n for k in keywords)

    mask = ~anomalies.apply(
        lambda row: should_exclude_impact_sockets(row['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'], row['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞']),
        axis=1
    )
    anomalies = anomalies[mask]
    anomalies = anomalies.sort_values('similarity_score')

    # ‚≠ê –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º original_count –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(
        f"üîç –ù–∞–π–¥–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π –∏–∑ {original_count} –∑–∞–ø–∏—Å–µ–π ({len(anomalies) / original_count * 100:.1f}%)")

    if len(anomalies) > 0:
        writer.save_anomalies(anomalies, 'data/ultra_enhanced_anomalies.csv')
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–π: {anomalies['similarity_score'].mean():.3f}")
        print("\nüîç –¢–û–ü-5:")
        for i, (_, row) in enumerate(anomalies.head(5).iterrows(), 1):
            print(f"{i}. –°—Ö–æ–∂–µ—Å—Ç—å: {row['similarity_score']:.3f}")
            print(f"   –ú–∞—Å—Ç–µ—Ä: {row['–ú–∞—Å—Ç–µ—Ä-–ø–æ–∑–∏—Ü–∏—è'][:50]}...")
            print(f"   –ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞: {row['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'][:50]}...")
    else:
        print("‚úÖ –ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")


if __name__ == "__main__":
    main()
